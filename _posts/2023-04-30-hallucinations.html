---
layout: post
title: "Shrooms of Your Language Models: What is Causing Hallucinations?"
subtitle: "What are the common reasons that cause your generative language models to hallucinate?"
date: 2023-04-30 23:45:13 -0400
background: '/img/posts/01_hallucinations.jpg'
---

<p><i>It’s still magic even if you know how it’s done. – Terry Pratchett</i></p>

<p>Hallucinating language models is bad news. Period.
Conventional concerns like user trust and model performance make hallucinations a concern. Add to this, the potential for life-threatening consequences like a translating model that hallucinates and messes up the medicine dosage or instructions of use. Moreover, a language model can be prompted to divulge sensitive information that was present in the training corpus. This too, is a form of hallucination since the model generated text that was not faithful to the source content. Here, the assumption that the source input (prompt) did not contain personal information was treated as a fact. The model output was no longer true to this fact.</p>

<p>The hallucinated text gives the impression of being fluent and natural despite being unfaithful and nonsensical. It appears to be grounded in the provided context, although it is hard to specify or verify the existence of such contexts. Similar to psychological hallucination which is hard to tell apart from other “real” perceptions, hallucinated text is also hard to capture at first glance.</p>
